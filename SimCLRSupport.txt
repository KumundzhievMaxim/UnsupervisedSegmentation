Created by Maksim Kumundzhiev. 
Support file with SimCLR architecture representation.


SimCLR: A Simple Framework for Contrastive Learning of Visual Representations.
- Original GitHub repo: https://github.com/HobbitLong/SupContrast
- Original paper: Supervised Contrastive Learning (Google Research, MIT, 2020)
- Habr with explonation: https://habr.com/ru/company/ods/blog/505040/

Contrastive Learning
- https://arxiv.org/pdf/1505.05192.pdf
- https://dyakonov.org/2020/06/03/%D1%81%D0%B0%D0%BC%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-self-supervision/
- Self-Supervied video expalined: https://youtu.be/GBgns_U9mmI
- List of papers by topic: https://github.com/jason718/awesome-self-supervised-learning; https://github.com/Sungman-Cho/Awesome-Self-Supervised-Papers

###Types of Learning:
1. Supervised
2. Semi-Supervised
3. Unsupervised
4. Self-Supervised 
5. Reinforsement

###Tasks which solved by Self-Supervised learning:
1. Предсказание контекста / Predicting (spatial) context: https://arxiv.org/pdf/1505.05192.pdf
2. Определение поворота / Predicting image rotation: https://arxiv.org/pdf/1803.07728.pdf
3. Образец / Exemplar: https://arxiv.org/pdf/1406.6909.pdf
4. Решение головоломки (Jigsaw): https://arxiv.org/pdf/1603.09246.pdf
5. Глубокая кластеризация (DeepCluster): https://arxiv.org/abs/1807.05520


###Types of tasks in Self-Supervised Learning.
Предварительная задача (pretext task) – задача с искусственно созданными метками (псевдо-метками), на которой обучается модель, чтобы выучить хорошие представления (representations) объектов.  Например, есть надежда, что в нейронной сети на предварительной задаче обучатся начальные и средние слои, их потом замораживают и обучают последние слои.

Псевдо-метки (pseudo labels) – метки, которые получаются автоматически, без ручной разметки, но обучение которым способствует формированию хороших представлений.

Последующая задача (downstream task) – задача на которой проверяют качество полученных представлений. Почти во всех экспериментах в статьях по самообучению на полученных признаковых представлениях в последующих задачах обучают простые модели: логистическую регрессию или метод ближайшего соседа. Таким образом, самообучение – это направление в глубоком обучении, которое стремится сделать глубокое обучение процедурой предварительной обработки данных, т.е. сети нужны для формирования признаков, они обучаются на дешёвой разметке больших наборов изначально неразмеченных данных, а сама задача решается простой моделью.

###Hacks:
В реализации метода есть несколько «хаков», которые мы здесь опишем, поскольку они часто применяются в самообучении. На рис. 2 видно, что патчи расположены с зазором и случайным смещением. Связано это с тем, чтобы усложнить задачу, поскольку человек может из кусочков собрать пазл совершенно не задумываясь о смысле изображённого (смотря на то, как согласуются стыки), а нам бы хотелось, чтобы нейросеть научилась не находить похожие стыки, а именно «понимать», что изображено. Другой приём – в подобных методах «сильно портят цвета», например, здесь два случайных канала забивались шумом. Связано это с законами оптики (см. хроматическая аберрация): все современные снимки сделаны с помощью фото/видео-оборудования и у них наблюдается смещения разницы средних по RGB-каналам при переходе от центра к краям (например, можно натренировать сеть, которая довольно точно будет определять, из какой части изображения вырезан патч). Чтобы сеть не использовала эту особенность при решении предварительной задачи, как раз и портят каналы.

###Supervised Contrastive Learning
Предлагают новый лосс на замену cross-entropy: модифицируют batch contrastive loss. Эмбеддинги одинаковых классов получаются схожими, эмбеддинги разных классов — разными. Ещё используют большой размер батча и нормализованные эмбеддинги.


Модельки получаются крутыми (по сравнению с другими моделями, использующими autoaugment) и робастными. В общем по сути похоже на triplet loss, но в нем не по одной паре позитивных-негативных примеров, а много позитивных и много негативных.


Cross-entropy активно используется в задачах с картинками, но у ней есть ряд известных проблем. Было сделано много попыток для их исправления, такие как label smoothing, self-distillation, mixup и другие.


Авторы предлагают новый лосс, основная суть которого заключается в том, что нормализированные эмбеддинги одного класса должны быть ближе друг к другу, чем эмбеддинги разных классов. Этот лосс является по факту продолжением идеи contrastive objective functions, которые хорошо работают в self-supervised learning. Ну и логично, что есть связь с metric learning.


В contrastive лоссах есть две "противоположные силы". Первая "сила" тянет якорь ближе к одним точкам, вторая толкает его подальше от других точек. Первые — позитивные семплы, вторые — негативные. Одно из ключевых новшеств статьи — в том, что у нас будет много позитивных и много негативных семплов на каждый якорь, причём выбираются лучше чем в предыдущих подходах.


Что полезного дает статья:


вот этот самый лосс;
демонстрация качества и робастности на Imagenet;
этот лосс менее чувствительный к значениям гиперпараметров;
доказывают аналитически, что градиент лосса способствует обучению на hard positive и hard negative;
показывают, что triplet loss — частный случай предложенного.


 
